output:
  dest:
    - fileName: file_convert.parquet
      srcType: S3
      fileType: txt
      filePath: s3://testing-input-output-bucket/output-glue     # S3 location where user wants to store data.
      argName: outputfile 
functional:
  process:
    mysql_source_endpoint:
      dataType: string
      value: <source_endpoint>                    # Hostname of the publicly accessible RDS. (source RDS) 
    mysql_source_port:
      dataType: string
      value: <mysql_source_db_port>               # MySQL RDS port.(3306)
    mysql_source_host_username:
      dataType: string
      value: <username>                           # Username of source Mysql RDS.
    mysql_source_host_password:
      dataType: string
      value: <password>                           # Password of source Mysql RDS.
    source_database:  
      dataType: string
      value: <database_name>                      # Database name from which you want to trasfer/load data.
    source_table:
      dataType: string
      value: <source_table_name>                  # Name of the Table user want to transfer to destination Database. 
    format:
      dataType: string
      value: csv                                  # csv | json | avro | xml | parquet
    columns:
      dataType: string
      value: [column1, column2, column3, column4] # list of column names user wants to load into target database/table. 
deployment: 
  type: dataloader
  config:  
    jobType: mysql_to_s3                       # mysql_to_mysql | mysql_to_s3 | gluejob 
    dbName: database_name                      # future deployment  
    crawlerName: crawler_name                  # future deployment  
    dataTarget: s3 path of data                # future deployment  
    jobName: lgSearceGlueAddJob                 
    jobVersion : v1
    jobMaxCapacity : 10
    jobTimeout : 60
    jobExecuteType : glueetl

general:
  debug: no
  level: 1
  scriptFile: mysqlgluescript.py                # Glue Dataloader scripts which is used to transfer/load data from RDS to S3.
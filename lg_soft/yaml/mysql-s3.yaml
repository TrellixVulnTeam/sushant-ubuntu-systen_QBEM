metaData:
  name: "Glue Module"
  description: "Glue Job"
  config: default
  version: 1.0
  team: "NLP"
input:
  source:
    - filePath: 's3://lppdatasource103424-devthree/'
      name: "batch_data_cleansed.csv" 
      srcType: S3     
      argName: "inputfile"
      fileType: csv  
output:
  dest:
    - fileName: file_convert.parquet
      srcType: S3
      fileType: txt
      filePath: s3://testing-input-output-bucket/output-glue
      argName: outputfile 
functional:
  process:
    mysql_source_endpoint:
      dataType: string
      value: <source_endpoint>
    mysql_source_port:
      dataType: string
      value: <mysql_source_db_port>
    mysql_source_host_username:
      dataType: string
      value: <username>
    mysql_source_host_password:
      dataType: string
      value: <password>
    source_database:
      dataType: string
      value: <database_name>
    source_table:
      dataType: string
      value: <source_table_name>
    format:
      dataType: string
      value: csv                                    # csv | json | avro | xml | parquet
    columns:
      dataType: string
      value: [column1, column2, column3, column4]
deployment: 
  type: dataloader
  config:  
    jobType: mysql_to_s3                    #mysql_to_mysql | mysql_to_s3 | gluejob 
    dbName: database_name                      #future deployment  
    crawlerName: crawler_name                  #future deployment  
    dataTarget: s3 path of data                #future deployment  
    jobName: lgSearceGlueAddJob                 
    jobVersion : v1
    jobMaxCapacity : 10
    jobTimeout : 60
    jobExecuteType : glueetl

general:
  debug: no
  level: 1
  scriptFile: mysqlgluescript.py
#  https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone_dist_script_mode.ipynb

frameworkType=="xgboost":

model_s3_location = "s3://sagemaker-ap-southeast-1-797237262327/sagemaker/DEMO-xgboost-inference-script-mode/DEMO-xgboost-inference-script-mode-2021-05-19-09-04-21/output/DEMO-xgboost-inference-script-mode-2021-05-19-09-04-21/output/model.tar.gz"

xgboost_model = XGBoostModel(model_data=model_s3_location, role=role, entry_point='/root/airflow/dags/modules/inference.py', framework_version="1.0-1", env={'MMS_DEFAULT_RESPONSE_TIMEOUT': '500'}, py_version='py3')

transformer = xgboost_model.transformer(instance_count=1, instance_type='ml.m4.xlarge')

input_data_path = "s3://sagemaker-ap-southeast-1-797237262327/sagemaker/DEMO-xgboost-inference-script-mode/train/abalone"

output_data_path = "s3://sagemaker-ap-southeast-1-797237262327/sagemaker/DEMO-xgboost-inference-script-mode/train/aboutput"

transform_job = sagemaker.transformer.Transformer(
		model_name = model_name,
		instance_count = 1,
		instance_type = 'ml.m4.xlarge',
		strategy = 'SingleRecord',
		assemble_with = 'Line',
		output_path = output_data_path,
		base_transform_job_name='inference-pipelines-batch',
		sagemaker_session=sagemaker.Session(),
		accept = "text/csv")

transform_job.transform(data = input_data_path,
		content_type = "text/libsvm",
		split_type = 'Line')
